{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f664e36",
   "metadata": {},
   "source": [
    "## Step 1: Set up a Perspective API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df9c73",
   "metadata": {},
   "source": [
    "### Find a unique API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2068ad0",
   "metadata": {},
   "source": [
    "MY API KEY: AIzaSyBWIx2SjeNJwB2SLFnL8_QK96ezEabhE5c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6abff",
   "metadata": {},
   "source": [
    "## Step 2: Explore the sample dataset to form hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccd5ba",
   "metadata": {},
   "source": [
    "### Visual inspection of the comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cfd37",
   "metadata": {},
   "source": [
    "Some toxic words found from sample_labaled_data just looking at the data: Genocide, Hell, vandalize, demon, arrogant, idiot, shit, unhappy, sucks, bomb, poor, miserable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec623805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_labeled_data = pd.read_csv(\"sample_labaled_data.csv\")\n",
    "\n",
    "# include toxic comments\n",
    "toxic_comments = sample_labeled_data.loc[sample_labeled_data[\"toxic\"] == \"yes\", \"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef0e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words that appear in the toxic comments\n",
    "toxic_words = []\n",
    "for comment in toxic_comments:\n",
    "    words = comment.split()\n",
    "    for word in words:\n",
    "        if len(word) > 3:\n",
    "            toxic_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185ab6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each toxic word\n",
    "toxic_word_freq = {}\n",
    "for word in toxic_words:\n",
    "    if word in toxic_word_freq:\n",
    "        toxic_word_freq[word] += 1\n",
    "    else:\n",
    "        toxic_word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0506bcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigger: 1467\n",
      "that: 1114\n",
      "faggot: 1049\n",
      "this: 1011\n",
      "boob: 1001\n",
      "poop: 987\n",
      "hate: 955\n",
      "your: 891\n",
      "youi: 886\n",
      "niggers: 884\n",
      "penis: 624\n",
      "stupid: 618\n",
      "balls: 555\n",
      "with: 513\n",
      "bitch: 512\n",
      "like: 507\n",
      "have: 498\n",
      "kill: 494\n",
      "must: 486\n",
      "vandal: 469\n"
     ]
    }
   ],
   "source": [
    "# Sort the toxic words by frequency\n",
    "sorted_toxic_words = sorted(toxic_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the 20 most common toxic words\n",
    "for word, freq in sorted_toxic_words[:20]:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddac6d6",
   "metadata": {},
   "source": [
    "## Step 3: Form hypotheses, Design and perform tests [4 MARKS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a19966",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00bbbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxicity score of the toxic comments: 0.7210722923562229\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = 'my-api-key'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    analyze_request = {\n",
    "        'comment': {'text': comment},\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "\n",
    "# Read the sample data CSV file\n",
    "sample_data = pd.read_csv('sample_labaled_data.csv')\n",
    "\n",
    "# Get toxic comments\n",
    "toxic_comments = sample_data[sample_data['toxic'] == 'yes']\n",
    "\n",
    "# Calculate the average toxicity score of the toxic comments\n",
    "toxicity_scores = []\n",
    "for comment in toxic_comments['comment_text']:\n",
    "    try:\n",
    "        score = predict_toxicity(comment)\n",
    "        toxicity_scores.append(score)\n",
    "    except:\n",
    "        # Ignore comments that language that is not supported by the API\n",
    "        pass\n",
    "    \n",
    "average_toxicity_score = sum(toxicity_scores) / len(toxicity_scores)\n",
    "\n",
    "print(f\"Average toxicity score of the toxic comments: {average_toxicity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec998e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "# See how many toxic comments were in the csv file\n",
    "print(len(toxicity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f629ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxicity score of the non-toxic comments: 0.18641444915142857\n"
     ]
    }
   ],
   "source": [
    "non_toxic_comments = sample_data[sample_data['toxic'] == 'no']\n",
    "\n",
    "# There are too many, so I will just 300 comments which is simliar to the number of toxic comments\n",
    "non_toxic_comments = non_toxic_comments.sample(n=300)\n",
    "\n",
    "# Calculate the average toxicity score of the non-toxic comments\n",
    "non_toxic_toxicity_scores = []\n",
    "for comment in non_toxic_comments['comment_text']:\n",
    "    try:\n",
    "        scores = predict_toxicity(comment)\n",
    "        non_toxic_toxicity_scores.append(scores)\n",
    "    except:\n",
    "        # Ignore comments that language that is not supported by the API\n",
    "        pass\n",
    "    \n",
    "average_non_toxic_toxicity_score = sum(non_toxic_toxicity_scores) / len(non_toxic_toxicity_scores)\n",
    "\n",
    "print(f\"Average toxicity score of the non-toxic comments: {average_non_toxic_toxicity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca43ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxicity score of the non toxic comments and toxic comments: 0.45374337075382576\n"
     ]
    }
   ],
   "source": [
    "threshold = (average_toxicity_score + average_non_toxic_toxicity_score)/2\n",
    "\n",
    "print(f\"Average toxicity score of the non toxic comments and toxic comments: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5315fad",
   "metadata": {},
   "source": [
    "So, I would choose the threshold as 0.45 because that is the average score of the toxic comments and non-toxic comments from the sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f56a0b",
   "metadata": {},
   "source": [
    "### Initial Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803e461",
   "metadata": {},
   "source": [
    "My hypothesis is that the longer the text, the more mistakes Perspective will make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e957790",
   "metadata": {},
   "source": [
    "###  Testing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaaced",
   "metadata": {},
   "source": [
    "So, I made my own data set : real_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2721161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 (i.e., toxic) accuracy for short comments = 0.9\n",
      "Class 0 (i.e., non-toxic) accuracy for short comments = 1.0\n",
      "Class 1 (i.e., toxic) accuracy for long comments = 0.5\n",
      "Class 0 (i.e., non-toxic) accuracy for long comments = 1.0\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "import pandas as pd\n",
    "\n",
    "# create a function to get toxicity score from Perspective API\n",
    "API_KEY = 'my-api-key'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    analyze_request = {\n",
    "        'comment': {'text': comment},\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "\n",
    "# load test data into a DataFrame\n",
    "test_df = pd.read_csv(\"real_dataset.csv\")\n",
    "\n",
    "# get the scores for each comment in the test data\n",
    "toxicity_scores = [get_toxicity_score(comment) for comment in test_df[\"Comment_text\"]]\n",
    "\n",
    "# classify each comment as toxic (1) or non-toxic (0) based on the threshold\n",
    "threshold = 0.45\n",
    "y_predicted = [int(score >= threshold) for score in toxicity_scores]\n",
    "\n",
    "# extract indices of examples for short and long comments\n",
    "lengths = test_df[\"Comment_text\"].apply(len)\n",
    "short_indices = lengths[lengths <= 50].index.tolist()\n",
    "long_indices = lengths[lengths > 50].index.tolist()\n",
    "\n",
    "# separate the actual labels and predicted labels for short and long comments\n",
    "y_actual_short = [test_df[\"toxic\"][i] for i in short_indices]\n",
    "y_predicted_short = [y_predicted[i] for i in short_indices]\n",
    "\n",
    "y_actual_long = [test_df[\"toxic\"][i] for i in long_indices]\n",
    "y_predicted_long = [y_predicted[i] for i in long_indices]\n",
    "\n",
    "# create a function to calculate class-wise accuracy\n",
    "def class_wise_acc(y_actual, y_predicted, class_label):\n",
    "    total_p = 0\n",
    "    total_n = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    for i in range(len(y_predicted)):\n",
    "        if y_actual[i] == class_label:\n",
    "            total_p += 1\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                TP += 1\n",
    "        else:\n",
    "            total_n += 1\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                TN += 1\n",
    "    if total_p == 0:\n",
    "        class_acc = 0\n",
    "    else:\n",
    "        class_acc = TP / total_p\n",
    "    return class_acc\n",
    "\n",
    "# calculate class-wise accuracy for short and long comments\n",
    "class_1_acc_short = class_wise_acc(y_actual_short, y_predicted_short, 1)\n",
    "class_0_acc_short = class_wise_acc(y_actual_short, y_predicted_short, 0)\n",
    "class_1_acc_long = class_wise_acc(y_actual_long, y_predicted_long, 1)\n",
    "class_0_acc_long = class_wise_acc(y_actual_long, y_predicted_long, 0)\n",
    "\n",
    "# print the class-wise accuracy for short and long comments\n",
    "print(f\"Class 1 (i.e., toxic) accuracy for short comments = {class_1_acc_short}\")\n",
    "print(f\"Class 0 (i.e., non-toxic) accuracy for short comments = {class_0_acc_short}\")\n",
    "print(f\"Class 1 (i.e., toxic) accuracy for long comments = {class_1_acc_long}\")\n",
    "print(f\"Class 0 (i.e., non-toxic) accuracy for long comments = {class_0_acc_long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd266b4",
   "metadata": {},
   "source": [
    "### Assess the Hypothesis based on your sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b13753",
   "metadata": {},
   "source": [
    "As you can see on the result, for short toxic comments, I got about 90% accuracy. However, for the long toxic commnets, I got about 50% accuracy which is low. Therefore, I would like to say that my hypothesis might be correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68db6bb",
   "metadata": {},
   "source": [
    "### How does a low sample size impact your results, and the conclusions we can draw from them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f05818",
   "metadata": {},
   "source": [
    "A low sample size could have a significant impact on my results and conclusions I drew from them. Since my sample had only 40 comments, this absolutely affected my accuracy calculation. If there were more samples, the accuracy of the long text could have been higher, or the accuracy of the short text could have been lower. Therefore, I realized that I need more samples when I work on a huge project later on, because low sample size can reduce the reliability of my conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249151af",
   "metadata": {},
   "source": [
    "## Step 4: Write about your results [4 MARKS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67383174",
   "metadata": {},
   "source": [
    "### What biases do you think might exist in the model based on intuitions or public documentation about how the model was created?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27430f",
   "metadata": {},
   "source": [
    "There are many possible sources of bias that can exist in the model. For example, looking at step 2, the word, \"nigger\", was the most common toxic word. However, the word that discriminates against Asians or other races could be not included in the model, which can be thought of as bias. Also, there can be some gender-related words such as Feminism. And there can be the possibility that these gender-related words can be thought of as toxic because the comments with those words usually come with a lot of controversies. We need to test the model more, but there could be more possible ways the biases exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ae632",
   "metadata": {},
   "source": [
    "### What were your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bad392",
   "metadata": {},
   "source": [
    "My hypothesis was that Perspective AI would make more mistakes on longer texts. Because the longer the text, the more positive and negative words are mixed, and I thought this fact would make it difficult for Perspective API to determine whether the text is toxic or not. First, to test this, we set the threshold as the average of the toxicity scores of toxic texts and the toxic scores of non-toxic texts obtained from the sample data. And, based on this threshold, I tested long toxic text, long non-toxic text, short toxic text, and short non-toxic text. The result says that longer toxic text has significantly lower accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd0d04",
   "metadata": {},
   "source": [
    "### What theories do you have about why your results are what they are?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c9b2b",
   "metadata": {},
   "source": [
    "As I said in my hypothesis, Perspective API really made more mistakes on the longer texts. However, the result could be different if I had more sample size. Therefore, this does not mean that my hypothesis is absolutely correct, but that it is worth exploring more about Perspective APIs making more mistakes on longer texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5be4f",
   "metadata": {},
   "source": [
    "### Personal Comment on the result overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47b96f",
   "metadata": {},
   "source": [
    "I've explored the accuracy of text's length this time, but I know that in order to evaluate this Perspective API, I still have to judge more elements. Gener-related words may cause more mistakes, or words referring to a particular race may cause more mistakes. In particular, I wonder the accuracy of Perspective API for text combined with other languages. Different languages often have different meanings, even if they have the same meaning in dictionary. Therefore, I wonder if Perspective API can understand the meaning of each country's cultural and social languages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
